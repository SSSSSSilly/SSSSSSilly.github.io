<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>conclusion</title>
      <link href="/2024/04/22/conclusion/"/>
      <url>/2024/04/22/conclusion/</url>
      
        <content type="html"><![CDATA[<h2 id="总结与反思"><a href="#总结与反思" class="headerlink" title="总结与反思"></a>总结与反思</h2><p><strong>博客主题及选取原因：</strong>该博客的风格整体体现出简单高级的感觉。</p><p><strong>页面布局及设计思路：</strong>为了使页面更加的鲜活，加入了个人照片，体现出博客的丰富性，且更加吸引人。并且引入文章的分类很文章列表，能更有效的管理文章。</p><p><strong>实现及技术选择：</strong>Hexo：是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他标记语言）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p><p><strong>博客制作过程中遇到的问题及其解决方法：</strong></p><p>问题1：本地运行正常，但网页运行无法加载CSS样式，只剩下文本</p><p>解决方法：检查缓存和文件路径两个要素，发现是没有清理缓存，在hexo clean清除完成之后，hexo g -d部署后就能正常显示。</p><p>问题2：系统提示error: RPC failed; result&#x3D;22, HTTP code &#x3D; 403</p><p>fatal: ‘username.github.io’ does not appear to be a git repository</p><p>解决方案：检查之后发现git配置错误，重新配置git并添加公钥之后，错误解决。</p><p>问题3：问题始终无法正常显示。</p><p>解决方案：从文件路径，markdown语法书写方式两个方面确认，发现是文件路径问题导致无法将图片文件加载到public文件夹中，修改后解决问题。</p>]]></content>
      
      
      
        <tags>
            
            <tag> conclusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost</title>
      <link href="/2024/04/22/imputing-out-of-vocabulary-embeddings-with-love-makes-language-models-robust-with-little-cost/"/>
      <url>/2024/04/22/imputing-out-of-vocabulary-embeddings-with-love-makes-language-models-robust-with-little-cost/</url>
      
        <content type="html"><![CDATA[<p><strong>Summary：</strong><br />State-of-the-art NLP systems represent inputs with word embeddings, but these are brittle when faced with Out-of-Vocabulary (OOV) words. To address this issue, we follow the principle of mimick-like models to generate vectors for unseen words, by learning the behavior of pre-trained embeddings using only the surface form of words. We present a simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language model (such as BERT), and makes it robust to OOV with few additional parameters. <br />Extensive evaluations demonstrate that our lightweight model achieves similar or even better performances than prior competitors, both on original datasets and on corrupted variants. <br />Moreover, it can be used in a plug-and-play fashion with FastText and BERT, where it signifificantly improves their robustness.</p><p><strong>Background：</strong><br />词嵌入将单词表示为向量。它们在神经网络方法中发挥了重要作用，为许多自然语言处理（NLP）任务带来了令人印象深刻的性能提高。这些方法使用一个固定大小的词汇表。因此，他们只能处理在训练中看到的单词。虽然这在许多基准数据集上都很有效，但真实单词语料库通常噪音更大，并且包含了Out-of-Vocabulary（OOV）单词，即罕见单词、领域特定单词、俚语单词和有拼写错误的单词，这些在训练中没有出现过。使用看不见的单词，模型的性能会很差，微小的字符扰动会改变模型的预测。简单的实验如下图表明，在数据集上添加拼写错误，大大降低了文本分类模型的性能。<br /><br />为了缓解这一问题，率先在大规模数据集上使用形态特征（子词标记）预先训练单词嵌入。在这一方向上最突出的作品之一是FastText，它将字符级别的n-grams应用到skip-gram模型中。使用FastText，看不见的单词的向量可以通过将n-grams向量相加来计算。然而，这些子字级模型带来了巨大的成本：从头开始的预训练需求和高内存占用。因此，已经开发了一些更简单的方法，例如，MIMICK、BoS和KVQ-FH。它们只使用单词的表面形式，通过学习预先训练过的嵌入来生成看不见的单词的向量。<br />为了缓解OOV的问题，两个主要的挑战仍然存在。首先，模型仍然处于复杂性和性能之间的权衡：原始模拟是轻量级的，但不能一致地产生高质量的词向量。BoS和KVQ-FH虽然获得了更好的词表示，但还需要更多的参数。其次，这些模型不能与现有的预先训练过的语言模型一起使用，如BERT。然而，这些模型在该领域取得了很大的进步。到目前为止，这些高性能的模型在处理罕见词汇、拼写错误和领域特定词汇时仍然很脆弱。<br />我们设计了一个新的对比学习框架来学习预训练嵌入的行为，称为LOVE，学习Out-of-Vocabulary嵌入。我们的模型建立在节省内存的字符和子词混合输入上，而不是n-gram字符。它通过一个轻量级的位置注意模块对这个输入进行编码。在训练过程中，LOVE使用了新类型的数据增强和硬负向生成。然后，该模型能够产生高质量的单词表示，对字符扰动具有鲁棒性，而只消耗现有模型成本的一小部分。例如，具有6.5M参数的LOVE可以获得与具有超过900M参数的原始FastText模型相似的表示形式。更重要的是，我们的模型可以用于即插即用的模式。<br /><br />我们的轻量级OOV模型LOVE学习了预先训练过的嵌入数据（例如，FastText和BERT)的行为，然后能够为看不见的单词输入向量。LOVE可以以即插即用的方式增强现有单词表示的健壮性。</p><p><strong>Related Work：</strong><br /><strong>Character-level Embeddings：</strong><br />为了解决OOV问题，一些方法在预训练期间将字符级特征注入单词嵌入中。这些方法的一个缺点是，它们需要从头开始在大规模语料库上进行预训练。因此，人们开发了更简单的模型，直接模拟训练良好的单词嵌入来输入OOV单词的向量。其中一些方法仅使用单词的表面形式来生成看不见的单词的嵌入，而其他人则使用表面和上下文信息来创建OOV向量。在这两种情况下，模型都需要过多的参数。<br /><strong>Pre-trained Language Models：</strong><br />目前，最先进的单词表示都是预先训练过的语言模型，如ELMo、BERT和XLnet，它们采用子词来避免OOV问题。然而，当面对罕见的单词和拼写错误时，BERT是很脆弱的。为了使BERT更强大，tigisBERT和CharBERT注入字符级特征，并对变体进行预训练。该方法可以显著提高BERT的性能和鲁棒性，但需要在大量数据上对自适应的transformer进行预训练。另一项关于消除拼写错误的工作建议在下游模型之前放置一个单词校正器，这是有效的和可重用的。该方法的主要缺点是，由单词校正器产生的错误会传播到下游任务。例如，将“aleph”转换为“alpha”可能会打破数学陈述的含义。事实上，使用校正器这个词总是会导致BERT在SST数据集上的性能下降（0.5-2.0个百分点）<br /><strong>Contrastive Learning：</strong><br />对比学习框架通过将正对拉在一起并将负对分开，从未标记的数据中学习表示。对于训练，正对通常是通过取同一样本的两个随机增广版本，并将一个小批中的其他增广例作为负例来获得的（Chen et al.，2017,2020）。最广泛使用的损失是信息系数损失（或对比损失）。虽然许多方法采用对比学习来表示句子，但到目前为止还没有应用于单词表示。</p><p><strong>Method：</strong><br />LOVE (Learning Out-of-Vocabulary Embeddings) 利用对比学习的原则，最大限度地提高目标和生成的向量之间的相似性，并推动负对分开。对框架的概述如下图所示。它的灵感来自视觉表示学习的工作，但不同之处在于，其中一个正对是从预先训练的嵌入中获得的，而不是使用两个增强版本。在这个框架中，我们采用了五种新的单词级增强类型和一个轻量级的位置注意模块。此外，我们发现在训练过程中添加硬负数可以有效地产生更好的表示。我们在编码器层之后删除了非线性投影头，因为它的改进是特定于视野中的表示质量的。此外，我们的方法不是一个无监督的对比学习框架，而是一个有监督的学习方法<br />我们的框架从原始词汇表中提取一个单词，并使用数据增强来产生对它的破坏。例如，“misspelling”在删除一个字母“s”后就变成了“mispelling”。接下来，我们从预先训练好的原始单词的嵌入中获得一个目标向量，并为被损坏的单词生成一个向量。这两个向量是一对正样本，我们最大限度地提高它们之间的相似性，同时使负对的距离（同一小批中的其他样本）尽可能大。如前所述，我们使用对比损失作为一个目标函数。<br />具体来讲，有一个词表 V _，_以及它的embedding矩阵 W ,对于词 w ，他的word vector是 uw 。Mimick模型的目的是为了任何一个不属于 V 的词 w 估计出一个 vw。训练的目标是最小化 uw 和vw对之间的期望距离。<br /><br />这里的 φ（·）是距离函数，比如欧氏距离或者余弦距离。向量vw 由下式生成：<br /><br />这里的 _ζ_（·） 根据单词的表面形式将 w 映射到子单元列表。在此之后，序列被喂进函数 φ（·）中，去生成vector，它可以是CNN、RNN或者一些简单的求和函数。<br />Input Method:<br />我们使用WordPiece，词汇量约为30000个，来获得输入词的有意义的子词。对于misspelling这个单词，这会产生{miss, ##pel, ##ling }。然而，如果我们只是交换两个字母（如打字错误），那么序列就会完全不同： {mi，##sp，##sell，##ing }。因此，我们同时使用字符序列和子词。我们通过重新定义所有的单词，只保留每个单词的基部形式，并删除带有数字的单词，来减少我们的词汇量。这将使词汇量从30 000个减少到21 257个，而不会过度降低性能。<br />Encoder：<br />现在让我们设计第3.1节中提到的函数φ（·)。我们正在寻找一个函数，可以编码本地特征和全局特征。局部特征是字符n-grams，它提供了对字符交换或遗漏等微小变化的鲁棒性。全局特征结合了局部特征，而不考虑其距离。<br />对于单词misspelling，一种前缀和后缀的模式mis+ing可以通过结合单词的开头和结尾的本地信息来得到。传统的cnn、rnn和自我注意不能同时提取这种局部和全局信息。因此，我们设计了一个新的位置注意模块。假设我们有一个上述的混合输入序列和一个相应的嵌入矩阵V∈R|V|×d，其中d是向量的维数。然后输入可以用一组向量列表来表示：，n是input的长度。为了提取局部信息，我们首先采用位置注意获得n-gram，然后将其输入传统的自注意层，以全局方式组合。这一点可以写成：<br /><br />在这里，SA是一种标准的多头自我注意，而PA是一种位置注意。WO∈RdV×dO是一个可训练的参数矩阵，其中dV为SA和PA值的维数，dO为值的维数。对于位置注意，我们采用绝对正弦嵌入来计算位置相关性：<br /><br />这里，P∈Rn×d为位置嵌入，WV∈Rd×dV为相应的参数。</p><p>Loss Function：<br />mimick的模型通常采用均方误差（MSE），它试图给出具有相同表面形式的单词相似的嵌入。然而，MSE只会将正词对拉得更近，而不会将负词对分开。因此，我们使用对比损失来代替。对比损失优化了两个关键特性：对齐和均匀性。对齐描述了正对之间的预期距离（接近度）：<br /><br />这里，ppos是正对的分布。均匀度测量学习到的表示是否在超球体中均匀分布。<br /><br />这里，pdata为数据分布，t &gt; 0为参数。这两个属性与我们期望的词表示相一致：正词对应该保持接近，负词对应该彼此远离，最后分散在超球面上。<br />Data Augmentation and Hard Negatives：<br />我们的正词对是通过数据增强来生成的，这可以通过使用现有的数据来增加训练样本的数量。我们使用了不同的策略来增加我们的训练样本的多样性：<br /><br /> (1)交换两个相邻的字符，(2)删除一个字符，(3)插入一个新的字符，(4)根据键盘距离替换一个字符，(5)用一个同义词替换原始单词。前四个增强功能最初是为了防止对抗性攻击。我们添加了同义词替换策略，以保持嵌入空间中词义相似的单词——这是表面形式无法实现的。具体来说，一组同义词是通过从FastText等预训练的嵌入中检索最近的邻居来获得的。<br />负词对通常是从小批量处理中随机选择的。然而，我们训练我们的模型对硬的否定（或困难的否定）具有特别的弹性，即，具有相似的表面形式但含义不同的单词(e.g., misspelling and dispelling）。为此，我们通过选择非同义词且编辑距离较小的单词对，将其中一定数量的硬负样本添加到mini-batch中（目前为3个）。<br />Mimicking Dynamical Embeddings:<br />预先训练的语言模型和BERT基于特定的上下文动态生成单词表示，这不能直接模拟。为此，我们有两种选择：我们可以在多层注意之前学习BERT中输入嵌入的行为，或者模拟静态蒸馏嵌入。<br />我们以BERT为例来解释这两种方法。假设我们在应用到一个句子后有一个子单词序列： 。对于子词序列W，BERT首先将其表示为子词嵌入列表：。我们将这个称为静态表示BERT的输入嵌入，我们可以使用我们的模型来模拟这部分的行为。我们称这种方法为模拟输入嵌入。为了便于实现，我们只从没有被分割成的单词中学习。在此步骤之后，BERT对输入嵌入Ein应用一个多层多头注意，从而产生一个上下文表示每个子字：。然而，这些上下文表征根据输入的句子而不同，我们不能直接从中学习。相反，我们选择模仿从BERT中提取的静态嵌入，这是通过汇集不同句子中单词的上下文嵌入（最大或平均)来获得的。我们把这种方法称为模拟蒸馏嵌入的方法。后者允许更好的单词表示，而前者不需要在大规模语料库上进行训练。我们的实证研究表明，模拟蒸馏嵌入的性能仅略好一些。因此，我们决定学习BERT的输入嵌入，这既简单又有效。<br />即插即用：<br />模型的一个关键优点是，它可以用作其他模型的插件。对于像FastText这样具有静态单词嵌入的模型，可以简单地使用该模型来生成看不见的单词的向量。对于像BERT这样具有动态单词嵌入的模型，如果一个单词被标记化为几个部分，例如misspelling&#x3D; {miss，##pel，##ling }，我们认为它看作一个OOV单词。然后，我们用模型在注意层之前产生的单一嵌入来替换子词的嵌入。</p><p><strong>Experiments：</strong><br />评价词的表征主要有两种方法：内在的和外在法。内在评价直接度量单词之间的句法或语义关系，例如，词集群中的单词相似性。外部评估是衡量了单词嵌入作为下游任务的输入特征的性能，例如，命名实体识别（NER）和文本分类。一些研究表明，内在和外在评价结果之间没有一致的相关性。因此，我们通过内在和外在的指标来评估表示。具体来说，我们使用了8个内在数据集（6个单词相似度和2个单词聚类任务）：RareWord、SimLex、MTurk、MEN、WordSim、Simverb , AP and BLESS。我们使用了四个外部数据集（2个文本分类和2个NER任务）：SST2、MR、CoNLL-03和BC2GM。值得注意的是，RareWord数据集包含许多长尾词，而BC2GM是一个特定于领域的NER数据集。所有的数据扩充和类型错误模拟都由NLPAUG1实现。<br />上表显示了8个内在任务的实验结果。与其他类似模拟的模型相比，我们的模型在8个数据集上获得了最好的平均分数，同时使用了最少的参数数量。具体来说，我们的模型在5个单词相似性任务上表现最好，在单词聚类任务上表现第二。尽管我们的模型和原始的FastText之间存在差距，但我们发现我们的性能是可以接受的，因为我们的模型要小100倍。<br /><br />上表显示了在SST2和CoNLL-03数据集上的性能。我们观察到，该模型可以提高原始嵌入的鲁棒性，而不降低其性能。此外，我们发现，与其他常用的方法相比，我们的模型对看不见的单词更健壮：一个通用的UNK标记或神经网络的字符级表示。<br /><br />上图表示了鲁棒性检查不同的策略。FastText+LOVE对SST2和CoNLL-03数据集都有一致的改进。与此同时，LOVE在原始数据集上的性能。<br /><br />为了验证我们的混合输入策略的效果，我们将其与其他两种方法进行了比较：只使用字符序列或只使用子词序列。从上表中可以看出，混合方法获得了更好的表示效果，任何去除字符或子字信息都会降低性能。<br />为了对输入序列进行编码，我们开发了位置注意模块（PAM），它首先提取类似ngram的局部特征，然后使用无距离限制的自注意进行组合。上表显示，PAM表现最好，这验证了我们在单词中合并局部和全局部分的策略。同时，相比之下。PAM的参数数量是可接受的。<br />上表中：，对比损失明显优于MSE。<br />在上表中，我们观察到去除我们的硬负数会降低性能，这表明了具有相似表面形式的语义上不同的词的重要性。LOVE使用了五种类型的单词增强功能。我们发现，删除这种增强不会降低单词相似任务的性能，而会导致文本分类任务（表5的最后一行）下降0.4分，其中数据增强有助于处理拼写错误。在图A3和图A4的附录中，我们进一步分析了在RareWord和SST2上的单个和复合增强的性能。我们发现，所有五种类型的组合都会产生最好的结果。<br /><br />（1）Linear：<br />其中，esub∈Rd是BERT的一个子字向量，eword∈Rd是我们的模型中生成的一个向量。W∈Rd是可训练的参数。<br />（2)Add：它们都可以在不降低原始能力的情况下给BERT带来一定程度的鲁棒性，这证明了我们的框架的有效性。其次，替代策略始终表现最好。我们推测，BERT不能为那些罕见的、拼写错误的单词恢复合理的意义，而我们生成的向量可以位于空间中原始单词的附近。第三，我们发现mimicking distilled embeddings表现最好，而mimicking input embeddings则很接近。考虑到第一种方法需要对大规模数据进行训练，mimicking input embeddings是我们选择的方法。</p><p><strong>Conclusion：</strong><br />我们提出了一个轻量级的对比学习框架，LOVE，来学习即使在面对词汇量之外的单词时也很健壮的单词表示。通过一系列的实证研究，我们已经表明，与其他类似模拟的模型相比，我们的模型（只有650万参数）可以在内在和外在评价上实现相似甚至更好的单词嵌入。此外，我们的模型可以以即插即用的方式添加到具有静态嵌入（如FastText）或动态嵌入（如BERT）的模型中，并在那里带来显著的改进。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Text-defense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CONNECTING LARGE LANGUAGE MODELS WITH EVOLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT OPTIMIZERS</title>
      <link href="/2024/04/21/connecting-large-language-models-with-evolutionary-algorithms-yields-powerful-prompt-optimizers/"/>
      <url>/2024/04/21/connecting-large-language-models-with-evolutionary-algorithms-yields-powerful-prompt-optimizers/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p><strong>ABSTRACT：</strong><br />大型语言模型（llm）在各种任务中表现出色，但它们依赖于精心制作的提示，而这往往需要大量的人力努力。为了自动化这一过程，在本文中，我们提出了一个新的离散提示优化框架，称为诱发提示，它借鉴了进化算法（EAs）的思想，因为它们表现出良好的性能和快速的收敛速度。为了使EAs能够处理离散的提示，这是一种需要连贯的、具有人类可读性的自然语言表达式，我们将llm与EAs联系起来。这种方法允许我们同时利用llm强大的语言处理能力和EAs的高效优化性能。具体来说，去掉任何梯度或参数，诱发提示从提示种群开始，基于进化操作符使用llm迭代生成新的提示，基于开发集改进种群。我们在31个数据集上，优化了包括GPT-3.5和Alpaca在内的封闭和开源llm的提示，包括语言理解、生成任务以及BIG-Bench Hard（BBH）任务。诱发提示的性能显著优于人工工程提示和现有的自动提示生成方法（例如，在BBH上高达25%）。此外，evo提示法还表明，将llm与EAs连接起来可以产生协同效应，这可以启发对llm与传统算法结合的进一步研究。</p><p><strong>Introduction：</strong><br />大型语言模型（LLMs）在多种自然语言处理（NLP）任务上表现出显著的表现。为了适应下游任务，只需在输入文本中添加一条指令，也称为离散提示符，即可引导llm执行所需的任务，而对计算成本的影响可以忽略不计。这种方法还消除了对llm中所有参数和梯度的需要，使其适用于具有api的llm，如GPT-3和GPT-4。尽管方便，LLMs对特定任务的表现受到提示的显著影响。因此，这种方法的关键挑战在于提示工程的设计，这已成为一种被称为提示工程的关键技术。鉴于不同语言模型和任务之间的提示符差异很大，提示性设计通常需要大量的人力努力和专业知识，以及主观的和相对有限的指导方针。<br />为了减轻人类在离散提示设计上的努力，以前的方法通常依赖于从llm的输出层访问token概率，这可能并不总是可以通过api访问。最近的一些工作考虑列举不同的提示并选择最好的提示，或修改当前的提示来改进它们，这些方法要么强调探索不同的提示，这可能导致优柔寡断和浪费资源，要么专注于利用当前确定的好提示，这可能导致停滞，并将搜索限制在局部最优。几种传统的无导数算法设计良好，在勘探和开发之间取得了良好的平衡。其中，进化算法（EAs）因其简单、高效、适合离散即时优化而突出，提示中的短语序列可以看作是典型EAs中的基因序列，使其与自然进化过程相容。<br />在本文中，我们借用了EAs的思想，并提出了一个离散的提示调优框架，即诱发提示。虽然EAs中的进化操作符通常是为序列而设计的，但它们倾向于独立地改变标记，以生成新的候选解决方案。不幸的是，这种方法忽略了令牌之间的连接，而这对于保持提示中的一致性和可读性至关重要。利用<strong>LLMs在NLP方面的专业知识和EAs的特殊优化能力，我们将这两种方法连接起来，其中LLMs根据进化操作符生成新的候选提示</strong>，而EAs指导优化过程以保留最优提示。<br />具体来说，基于几个初始提示，我们利用llm作为进化操作符来生成新的提示候选符，并保留了在开发集上具有更好性能的提示。对更新种群进行上述操作的迭代应用，以提高质量。通过精心设计进化算符和调整更新策略，可以用各种类型的EAs来实例化诱发提示。我们优化了两个不同的llm的提示，包括不同范围的神经语言理解和生成任务，以及具有挑战性的BBH任务，使用总共31个数据集。与人工设计的提示和以前的自动提示生成方法相比，诱发提示始终能得到更好的提示。</p><p><strong>Contributions：</strong><br />1、我们提出了一种新的连接llm和EAs的自动离散提示优化框架，称为evople提示，它具有以下优点： 1)不需要访问llm的任何参数或梯度；2)它在探索和开发之间取得平衡，从而获得更好的结果；3)生成的提示是人类可读的。<br />2、在31个数据集上进行的实验表明，与精心制作的提示以及现有的方法相比，诱发提示的有效性。针对情绪分类、主题分类、主观性分类、简化、总结和推理等常见任务，我们发布了通过诱发提示得到的最优提示。<br />3、我们证明了llm能够实现提供适当指令的多种类型的EAs。我们希望我们的探索将激发对llm与传统算法结合的进一步研究，为llm的新应用和创新应用铺平道路。</p><p><strong>Related Works：</strong><br /><strong>Prompts in LLMs ：</strong>提示法是在特定任务中使用llm的一种有效方法。然而，性能受到提示符的选择的严重影响。近年来，自动提示优化得到了广泛的关注。基于提示的连续方法，仅调整某些输入token的参数引起了大量关注。尽管这些范式具有有效的性能，但它们的两个缺点不容忽视： 1)连续提示的优化需要黑盒api无法访问的llmam参数。2)软提示往往缺乏可解释性。离散提示，简单地添加几个离散的标记，如“It是”，或特定于任务的描述性指令，如“将评论分为积极或消极”。，可以为输入文本提供一个交互界面，具有更好的可解释性的人，并在各种NLP任务中显示出良好的性能。<br /><strong>Discrete Prompts ：</strong>已经提出了各种方法来自动离散提示搜索和生成，而这些方法仍然依赖于输出层的梯度或标记概率。最近，考虑到下游任务中不同提示的高方差，一些工作集中于通过从一些候选提示中列举和选择最佳提示，主要通过重新抽样来增强。基于提示编辑的方法强调开发，这可能会导致局部优化。另一种方法是收集错误预测的案例，并分析相应的根本原因，以改进现有的提示，这也强调了开发利用。此外，这些方法被限制为具有标准答案的任务，不能直接应用于生成任务。我们提出的具有进化算法的诱发提示在不需要任何参数或梯度的情况下，在探索和开发之间取得平衡。<br /><strong>LLMs and Optimization Algorithms ：</strong>llm显示了作为黑盒优化器的潜力；然而，这种黑盒方法缺乏可解释性。一些研究表明，llm有能力模仿传统算法中的特定操作。例如，llm可以通过收集错误预测的样本，在离散空间中执行“梯度下降”。同时，已经证明LLMs可以模仿遗传算法（GA）中的突变或交叉操作符。Chen等人进一步集成了llm和GA用于神经结构搜索，而Lanzi&amp;Loiocono也引入了类似的游戏设计方法。我们的工作向前迈出了重要的一步，提出了一个将llm与进化算法连接起来的通用框架，该框架可以通过定制进化和选择过程实例化到不同的进化算法，从而扩大其在该领域的适用性和潜在影响。我们渴望这项工作，以激发结合llm和传统算法的更广泛的应用。</p><p><strong>Method：</strong><br />自动离散提示优化：<br /><br />当前的高级llm通常通过黑盒api进行交互，而梯度和参数是不可访问的。进化算法（EAs）是一种无导数的算法，具有特殊的精度和快速的收敛性。因此，我们考虑在离散即时优化中引入EAs。然而，为了生成新的候选解决方案，进化操作符通常会独立地编辑当前解决方案中的元素，而不考虑它们之间的连接。这使得在离散提示上应用进化操作符具有挑战性，因为这需要一致性和可读性。为了解决这一挑战，我们提出了一种协同的方法，将llm的自然语言处理专业知识与EAs的优化能力联系起来，称为诱发提示。具体来说，llm基于进化操作符生成新的候选提示，而EAs指导优化过程以找到最优提示。<br />为了在实践中实现诱发提示，有必要用一种特定的EAs算法对其进行实例化。EAs有各种各样的类型，在本文中，我们考虑了两种广泛使用的算法，包括遗传算法（GA）和微分进化（DE）。GA是最受重视的进化算法之一，DE自成立以来已成为复杂优化挑战的最广泛应用算法之一。下面，我们将首先概述所提出的诱发提示，然后分别用GA和DE实例化诱发提示。<br />FRAMEWORK OF EVOPROMPT：<br />EAs通常从N个解的初始种群开始（在我们的设置中是提示），然后在当前种群上使用进化算符（例如，突变和交叉）迭代生成新的解决方案，并基于适应度函数更新它。遵循典型的EAs，诱发提示主要包括三个步骤：<br /><strong>初始化种群：</strong>与大多数现有的忽略先验人类知识的自动提示方法相反，我们将可用的手动提示作为初始种群来利用人类的智慧。此外，EAs通常从随机解开始，导致一个多样化的种群，并避免陷入一个局部最优。因此，我们还在初始人群中引入了一些由llm生成的提示。<br /><strong>进化：</strong>在每次迭代中，诱发提示使用llm作为进化操作符，根据从当前总体中选择的几个父提示生成一个新的提示。为了实现这一点，我们为<strong>每个特定类型的EAs设计了突变和交叉操作符的步骤</strong>，以及相应的指令来指导llm基于这些步骤生成新的提示。<br /><strong>更新：</strong>我们在开发集上评估生成的候选提示，并保留那些具有优越性能的提示，类似于自然界中的适者生存。具体的更新策略可能会根据所使用的EAs的类型而有所不同。<br />当迭代的次数达到一个预定义的值时，该算法将停止。在算法1中概述了诱发提示的细节。当使用特定的EAs算法实例化诱发提示时，需要调整进化过程，而<strong>关键的挑战是在离散提示上设计进化操作符</strong>。<br />使用遗传算法的实例化：<br /><br />在遗传算法中，父解通常使用轮盘赌轮选择方法进行选择，并根据其适应度值进行指导。类似地，我们使用轮盘赌轮选择，从当前人群中选择两个父提示，基于他们在开发集中获得的性能分数。设si表示在包含N个提示的总体中，第i个提示的性能分数。选择第i个提示作为父提示的概率可以表示为<img src="https://cdn.nlark.com/yuque/0/2024/png/39032054/1710508524300-c28ed087-c827-49fa-9cc9-9594832a1240.png#averageHue=%23f6f3f0&clientId=u691cf207-c476-4&from=paste&height=25&id=u49c2bb45&originHeight=25&originWidth=117&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1920&status=done&style=none&taskId=u1d59634c-36db-40ec-a6df-5de42d7b52e&title=&width=117" alt="6fe89f7f3204b96ec3ff998b317f605.png">。<br />进化符合遗传算法框架，我们通过两个步骤生成一个新的候选提示： 1)在亲代提示之间进行交叉，产生一个新的后代提示；2)对子代提示进行突变，对某些元素引入随机改变。我们将这一两阶段的操作形式化为算法指令，以指导llm在算法1中实现Evo（·）。整个过程如图1所示。<br />我们采用了一种简单的选择策略来更新种群：在每次迭代中，诱发提示产生N个新的提示，这些提示与现有的N个提示的种群合并。随后，根据他们的分数，保留前N个提示，以形成更新的总体。因此，种群的整体质量不断提高，最终在最终种群中选择最好的一个作为最佳提示。<br />具有差异进化的实例化：<br /><br />在这里，我们从对DE的一些初步知识开始。与遗传算法不同，DE的解用数值向量表示。种群中的每个向量依次选择为一个基向量，记为x，随后进行突变和交叉。在突变过程中，从当前种群中随机选择的方案a生成突变方案y。该突变是通过将两个不同的、随机选择的解决方案b和c之间的比例差添加到a中来实现的，即y&#x3D;a+F（b−c），其中F是比例参数。<br />交叉是通过从基本解x或突变解y中选择向量中的每个参数来生成一个试验解<img src="https://cdn.nlark.com/yuque/0/2024/png/39032054/1710508788944-9e4974a0-4dcd-4d38-be4b-c809c1e96b10.png#averageHue=%23f9f6f2&clientId=u691cf207-c476-4&from=paste&height=22&id=uc51ba1b6&originHeight=22&originWidth=112&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1301&status=done&style=none&taskId=uc949ff55-3884-4910-bfd8-ab4c83fa3b9&title=&width=112" alt="be2ebcf167bbd1fae5a76bc0c10c700.png">。然后，如果x‘比x好，则用x’代替x。在逐步的进化中，DE以一个高质量的种群结束。DE的一个修改版本使用当前的最佳解决方案作为向量a来利用来自最佳解决方案的信息。<br />DE的进化过程可以解耦为三个步骤： 1) F（b−c）；2) y &#x3D; a + F（b−c）；3)x和y的交叉。在基于DE的诱发提示中，我们遵循三个步骤来设计进化过程，以及llm根据这些步骤生成新提示的相应指令，如图2所示：<br />1、受DE中的微分向量的启发，我们考虑只改变当前种群中两个随机选择的两个提示的不同部分（图2中的步骤1和步骤2）。当前人群中的提示被认为是当前最好的提示。因此，两个提示的共享组件往往对性能有积极的影响，因此需要予以保留。<br />2、DE的一个变体在突变过程中使用了当前的最佳向量，其中通过将微分向量的规模添加到当前的最佳向量中来生成一个突变向量。在此基础上，我们通过选择性地将当前最好的部分替换为突变的不同部分，从而生成突变提示。（图2中的第3步）。<br />3、交叉将基本提示符中的某些组件（即当前总体的候选组件）替换为从突变提示符中获得的片段。此操作结合了两个不同提示的特性，可能会创建一个新的和改进的解决方案（图2中的步骤4)。<br />按照标准DE，当前总体中的每个提示pi被选择为基本提示，然后使用图2中的指令生成相应的新提示p’i 。然后，保留得分较高的提示符，或pi或p’i 。因此，种群规模保持不变，而种群的总体质量则有所提高。</p><p>Experiments：<br />通过GPT-3.5执行进化操作符，我们使用开源Alpaca-7b和闭源GPT-3.5（文本-达文奇-003年）的诱发提示来优化提示。我们选择在开发集上分数最高的提示符，并在测试集上报告其分数。对羊驼报告的结果在3个随机种子上进行平均，并提供了标准偏差，而对于GPT-3.5，由于预算限制，我们报告了一个种子的结果。在我们的评估中，我们比较了诱发提示与三类基于提示的方法，具体如下：<br />**Manual Instructions (MI)**：这些作为特定任务的指导方针，并基于已建立的工作制定，特别引用Zhang等人（2023b）用于语言理解，Sanh等人（2021）用于总结，Zhang等人（2023c）用于文本简化。<br /><strong>PromptSource</strong> and <strong>Natural Instructions (NI) <strong>：这些存储库聚合了跨不同数据集的人类组成的提示。<br /><strong>APE和APO</strong>：APE采用迭代蒙特卡罗搜索策略，强调探索。我们复制它，并初始化与诱发提示相同大小的种群。APO将错误预测的实例作为“伪梯度”来迭代地细化原始提示，这强调了开发。我们以最优手动提示作为初始提示，在二进制分类任务上重现APO。<br /><strong>语言理解：</strong><br />数据集和设置我们首先进行实验语言理解任务在7个数据集来验证我们的方法，包括情绪分类（SST-2，MR，CR，SST-5），主题分类(AG’s News，TREC和主观性分类）。为了约束输出标签空间，我们在测试用例之前准备每个类一个示例的演示。<br /><br />主要结果表1显示： 1)与以往关于提示生成和人工书面指令的工作相比，基于GA和DE的诱发提示具有明显更好的效果。2)在情绪分类数据集上，诱发提示（GA）略优于诱发提示（DE）。当涉及到主题分类数据集时，evo提示符（DE）表现得更好。值得注意的是，在主观性分类任务（Subj）上，诱发提示符（DE）比其遗传算法有了很大的改进，获得了5%的准确率优势。这可能是由于当初始提示不是高质量时，DE逃避局部最优的特殊能力。<br /><strong>语言生成：</strong><br />数据集和设置为语言生成，我们评估我们的文本摘要和简化任务的诱发提示。对于总结，我们采用了SAMSum，这是一个具有挑战性和复杂的对话总结数据集，并报告了Alpaca-7b和GPT-3.5的ROUGE-1&#x2F;2&#x2F;L评分。为了简化文本，旨在在简化源文本的同时保留其原始含义，我们使用了资产数据集，这是一个以其多重参考翻译而闻名的基准。我们将SARI评分作为评价指标，这是一种基于n-gram的评分系统，广泛用于文本编辑任务。<br />主要结果汇总和简化结果见表2和表3。与人工设计的提示相比，Evopnicte实现了显著的性能提高，在Alpaca和GPT-3.5 API上的SARI分数提高了3分以上。此外，在评估场景中始终优于APE方法，表明所生成的提示能够有效地利用llm的功能来获得卓越的性能。此外，在总结任务中，诱发提示（DE）明显优于诱发提示（GA），同时在文本简化任务中表现出类似的性能。这表明，DE变体对于诸如摘要等更复杂的语言生成任务特别有效。<br /><strong>BIG BENCH HARD (BBH)：</strong><br />为了验证在不同的任务上我们的方法，我们应用BBH，包括23个具有挑战性的任务，需要多步推理。由于这些任务具有挑战性，我们专注于优化GPT-3.5的提示。我们从测试集中抽取一个子集作为开发集，并报告标准化分数1，并与提示符“让我们一步一步地思考”进行比较。在测试集上进行3次思维链演示。我们使用任务id来简化每个任务的表示，并删除一个，因为手动提示的准确率已经达到100%。<br /><br />evo提示符会为所有22个任务获得更好的提示（图3）。具体来说，诱发提示（DE）达到高达25%的改善，平均为3.5%，而诱发提示（GA）达到峰值为15%的改善，平均为2.5%。尽管对于某些任务，GA对应项的性能优于DE版本，但性能差距仍然相对较小（即约1%）。同时，在6个任务中，诱发提示（DE）超过诱发提示（GA）2%以上。因此，DE版本通常是处理这些具有挑战性的任务的一个很好的选择。<br />分析：<br />对于诱发提示（GA），我们默认应用轮盘赌轮选择策略来选择父母的提示，对后代有贡献。为了进一步探索各种选择策略的影响，我们将我们的方法与另外两种流行的策略进行了比较，即tournament和随机选择，如表4所示。我们观察到，使用轮盘赌轮的诱发提示（GA）获得了更高的分数，显示了这种选择方法的有效性。<br /><br />对于evo提示（DE），我们深入研究了使DE的进化操作符适应离散提示时的两个关键设计考虑： 1)不同部分的突变，2)选择当前性能最好的提示作为图2中的“提示3”。我们评估了这些设计选择对两个数据集： Subj，一个理解数据集，其中evo提示（DE）优于evo提示（GA），和资产，一个生成数据集，其中两个变体表现出相似的性能。<br />为了说明只突变不同部分的好处，我们将图2中的前两个步骤替换为指令“随机突变提示1和提示2”，以允许对提示1和提示2中的所有内容发生突变，在表5中表示为“所有”。同时，诱发提示中的原始设计，只改变不同的部分，记为“差异”。如表5所示，仅在不同部分上进行突变设计可以在两个任务中一致地获得性能提高。<br /><br />在DE算法的诱发提示（DE）中应用DE算法的一个变体，我们在当前总体中选择最佳提示作为图2中的提示3。我们通过以下设置来验证这一设计： 1)提示3从当前人群中随机抽样，在表5中表示为“随机”；2)通过让基本提示直接跨越突变的不同部分来消除提示3的使用（即删除图2中的步骤3），在Tabel 5中表示为“消除”。表5清楚地说明了引入提示符3的重要性。此外，选择最佳提示作为提示3比随机抽样更有效。<br /><strong>总体初始化：</strong><br />我们研究了初始种群质量对诱发提示的影响。我们进行了试点实验，根据提示在开发集上的性能对提示进行排序（手动设计或由GPT-3.5生成）。然后，我们选择底部、随机和顶部的提示以及它们相应的变化作为初始提示。这些变化是使用Zhou等人（2022）设计的重采样模板生成的，如附录B.2中的图4所示，该模板用于为初始化引入随机性。<br /><br />表6表明： 1)初始提示的</strong>精心设计并不必要</strong>，因为随机选择提示可以达到与选择性能最好的提示相似的性能；2)在选择性能最好的提示时，通过允许GPT-3引入随机性。产生变化可以导致整体性能的轻微提高；但是，当随机选择提示时，没有必要对诱发提示（DE）引入额外的随机性；3)当使用性能最好的初始提示时，诱发提示（GA）略优于诱发提示（DE）；但当从表现不佳的初始提示开始时，诱发提示（DE）优于诱发提示（GA），这表明当可用的手动提示质量不高时，DE是一个更好的选择。<br /><br /><strong>Conclusions：</strong><br />我们引入了诱发提示来优化离散提示，它将llm与进化算法连接起来。在31个数据集上的大量实验证明了诱发提示的优越性，比手动指令和现有方法产生一致的性能收益。此外，我们还验证了llm可以作为一个有效的、可解释的接口来实现遗传算法和DE等进化算法。虽然本研究集中在EAs上，但我们的方法的可扩展性为将llm应用于其他传统算法开辟了途径，如粒子群优化（PSO）、蚁群优化（ACO）和最近的质量-多样性（QD)优化算法。我们的发现旨在激发未来在llm和传统算法的交叉点上的研究，鼓励创新的应用。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Text-attack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HQA-Attack_ Toward High Quality Black-Box Hard-Label Adversarial Attack on Text</title>
      <link href="/2024/04/21/hqa-attack-toward-high-quality-black-box-hard-label-adversarial-attack-on-text/"/>
      <url>/2024/04/21/hqa-attack-toward-high-quality-black-box-hard-label-adversarial-attack-on-text/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p><strong>Summary:</strong><br />Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the perturbation rate. Then it leverages the synonym set of the remaining changed words to further optimize the adversarial example with the direction which can improve the semantic similarity and satisfy the adversarial condition simultaneously. In addition, during the optimizing procedure, it searches a transition synonym word for each changed word, thus avoiding traversing the whole synonym set and reducing the query number to some extent. Extensive experimental results on five text classification datasets, three natural language inference datasets and two real-world APIs have shown that the proposed HQA-Attack method outperforms other strong baselines significantly.</p><p><strong>Background：</strong><br />      深度神经网络（DNN）已经取得了巨大的成功，并且在以下领域非常受欢迎：各个领域，例如计算机视觉，自然语言处理，机器人技术等等。 尽管 DNN 模型取得了令人鼓舞的性能，但仍存在一些问题对其鲁棒性的担忧，因为有证据表明，即使是对输入的轻微扰动数据可以欺骗这些模型，使其产生错误的预测，这些令人不安的示例被命名为对抗性示例。 调查对抗性背后的生成原理例子似乎是提高神经网络鲁棒性的一种有前途的方法，这激励了关于对抗性攻击的研究。 大多数现有的对抗性攻击方法都集中在计算机上愿景并已得到充分探索。 然而，对文本数据的对抗性攻击是仍然具有挑战性，因为不仅文本数据空间本质上是离散且不可微的，而且另外稍微改变一下单词也可能会影响语法的流畅性和语义的一致性严重地。<br />基于受害者模型的可访问性级别，现有的文本对抗攻击方法可以被归类为白盒攻击和黑盒攻击。 对于白盒攻击时，假设攻击者拥有有关受害者模型的完整信息，包括训练数据、模型架构和参数。 因此，很容易将这种类型的攻击表述为优化问题并利用梯度信息生成对抗性示例。 然而，由于大多数模型开发者不可能公开所有的模型和数据信息，白盒攻击似乎过于理想化，在现实应用中无法很好地发挥作用。 对于黑匣子攻击时，假设攻击者只能访问预测结果，例如置信度分数或预测标签，这看起来更现实。 现有的黑盒文本攻击方法可以分为软标签设置和硬标签设置。 对于软标签方法，他们要求受害者模型提供置信度分数来计算每个问题的重要性单词，然后依次替换单词，直到生成对抗性示例。 然而，它是也不切实际，因为大多数真实的 API 不允许用访问置信度分数。 对于硬标签方法，他们只需要知道受害者模型的预测标签即可完成攻击任务，因而更具有实用性和前景。<br /><br />仅提出了少数几种方法处理黑盒硬标签文本广告对抗攻击任务，主要依靠基于启发式的或基于梯度的策略。 具体来说，HLGA是第一种硬标签对抗攻击方法，利用基于群体的启发式优化算法来制作合理且语义相似的对抗性示例。 然而，作为需要大量的对抗性可预测群体，基于群体的优化策略很容易陷入局部最优，HLGA不可避免地会消耗大量的查询次数。TextHoaxer将文本数据上的预算硬标签对抗性攻击任务制定为连续词嵌入中扰动矩阵的梯度优化问题空间。 LeapAttack通过设计一种可以互换离散替换和连续向量的新颖机制来利用基于梯度的优化。 虽然这些基于梯度的方法在一定程度上提高了查询效率，但仍然需要一些不必要的查询梯度估计不准确造成的。 此外，在预算紧张的场景下，查询效率低下会直接对语义相似度和扰动率带来严重的副作用。</p><p><strong>Contributions：</strong><br />为了缓解上述问题，我们提出了一个简单而有效的框架来产生高高质量黑盒硬标签对抗攻击，称为 HQA-Attack。 HQAAttack的概述如图1所示。“高质量”是指HQA-Attack方法可以生成在严格查询下具有高语义相似性和低扰动率的对抗性示例预算。 具体来说，HQA-Attck首先通过初始化生成对抗样本，然后依次将尽可能多的原始单词替换回去，从而缩小扰动率。最后，它利用同义词集进一步优化对抗性示例，方向为可以提高语义相似度并同时满足对抗条件。 此外，为了避免遍历同义词集，它会为每个更改的单词找到一个过渡同义词，从而在一定程度上减少了查询次数。 八个公共基准的实验结果数据集和两个真实的API（谷歌云和阿里云）证明HQA Attack在语义相似度和扰动率方面比其他强基线表现更好在相同的查询预算下。</p><p><strong>Method：</strong><br />软标签设置中的对抗性攻击依赖于所有类别的概率分布生成对抗性例子。 一系列策略利用贪心算法制作对抗性示例，首先确定单词替换顺序并贪婪地替换在此顺序下的每个单词。 TextFooler首先根据删除每个单词后的预测变化，然后根据单词重要性，直到生成对抗性示例。 类似地，TextBugger计算通过比较预测分别得出句子的重要性和单词的重要性移除它们之前和之后。 此外，还有一些方法使用组合生成对抗性示例的优化算法。<br />硬标签设置中的对抗性攻击只允许访问预测的标签，这看起来更具有挑战性和实用性。 HLGA是第一个硬标签文本对抗攻击方法。 它用随机初始化和搜索空间缩减以获得初期的对抗性示例，然后使用遗传算法包括变异、选择和交叉三种操作进一步优化对抗性的例子。 虽然 HLGA 可以生成具有高语义的对抗性示例与原始示例相似且扰动率低，需要维持较大的候选每次迭代都会设置，这会浪费大量的查询。 为了缓解这个问题，TextHoaxer使用词嵌入空间来表示文本，并引入扰动矩阵和新颖的目标函数由语义相似项、成对扰动约束组成和稀疏约束。 通过使用基于梯度的策略来优化扰动矩阵，TextHoaxer 可以在紧张的预算内生成适当的对抗性示例。 LeapAttack 是另一种基于梯度的方法。 随机初始化后，它通过以下方式优化对抗性示例不断地将示例移近决策边界，估计梯度并找到更新对抗性例子的正确词语。 在附录 K 中，我们进一步讨论了一些潜在的硬标签文本对抗攻击的应用场景。<br />在本文中，我们关注黑盒硬标签文本对抗攻击，即攻击者只能从受害者模型中获取预测的标签来生成对抗的例子。具体来说，给出了一个原始的例子<img src="https://cdn.nlark.com/yuque/0/2024/png/39032054/1713082784676-4ef96ee8-1fe1-406a-91df-432e3cb307ca.png#averageHue=%23f9f6f3&clientId=u718981f7-f8e3-4&from=paste&height=24&id=uce0e5c0b&originHeight=24&originWidth=152&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1939&status=done&style=none&taskId=ud5ac983b-6e88-4532-89c1-6683b59d733&title=&width=152" alt="image.png">有地面真实标签<img src="https://cdn.nlark.com/yuque/__latex/bf98c0ddcbe9c1e535f767c78c3aa813.svg#card=math&code=y&id=UKSH5">，其中<img src="https://cdn.nlark.com/yuque/__latex/d99fd2df7b5f652a4b7fc593fb9df750.svg#card=math&code=w_i&id=ukk5k">是第<img src="https://cdn.nlark.com/yuque/__latex/2443fbcfeb7e85e1d62b6f5e4f27207e.svg#card=math&code=i&id=i9fX0">个单词，<img src="https://cdn.nlark.com/yuque/__latex/df378375e7693bdcf9535661c023c02e.svg#card=math&code=n&id=bE0ml">是<img src="https://cdn.nlark.com/yuque/__latex/712ecf7894348e92d8779c3ee87eeeb0.svg#card=math&code=x&id=Ioh45">中的单词总数。本任务旨在建立一个敌对的例子<img src="https://cdn.nlark.com/yuque/0/2024/png/39032054/1713083011247-8c783361-4561-4e96-b2b8-86b8668fec1f.png#averageHue=%23f9f6f2&clientId=u718981f7-f8e3-4&from=paste&height=23&id=u27c6b4e4&originHeight=23&originWidth=170&originalType=binary&ratio=1&rotation=0&showTitle=false&size=1987&status=done&style=none&taskId=ud85e2b1f-1a08-48c9-b633-1aba79a5cda&title=&width=170" alt="image.png">。通过将包含<img src="https://cdn.nlark.com/yuque/__latex/d99fd2df7b5f652a4b7fc593fb9df750.svg#card=math&code=w_i&id=vF61T">本身的同义词集<img src="https://cdn.nlark.com/yuque/__latex/f68a44577aeba338f812a2315c0c5302.svg#card=math&code=S%28w_i%29&id=WBxqG">中的同义词<img src="https://cdn.nlark.com/yuque/__latex/8c114c023858474e0051b31ac447af72.svg#card=math&code=w%27_i&id=lPQcm">替换为原始词<img src="https://cdn.nlark.com/yuque/__latex/d99fd2df7b5f652a4b7fc593fb9df750.svg#card=math&code=w_i&id=bYx5e">，从而误导受害者模型<img src="https://cdn.nlark.com/yuque/__latex/18f3c2855f0e85a1ac2257f64d917144.svg#card=math&code=f&id=QM0bb">输出一个不正确的预测结果：<br /><br />其中，等式(1)可以看作是对抗性的情况。可能存在几个对抗性的例子可以满足等式(1)，但最优的对抗性例子x˚是在所有候选样本中与原始例子x语义相似性最高的例子。正式地，<br /><br />其中，是和之间的语义相似性。表1详细总结了符号说明。<br /><br />为了完成黑盒硬标签文本对抗攻击任务，我们遵循以前的工作首先利用随机初始化来生成对抗性示例。 具体来说，考虑原始示例中词性 (POS) 为名词、动词、副词和形容词的每个单词 ，我们随机选择一个同义词来自  的同义词集  作为 的替换，和重复此过程，直到生成对抗性示例 满足对抗条件。显然，使用随机初始化来生成对抗样本通常需要改变原始示例中存在多个单词，因此必然导致语义相似度较低扰动率大。 为了缓解上述问题，我们尝试在第次迭代中迭代优化原始例和对抗性例之间的语义相似性，其中和为迭代总数。<br />为了提高生成的对抗样本的语义相似度，之前的工作首先以每个原词带来的语义相似度提升作为衡量标准决定替换顺序，然后在每次迭代中不断放回原来的单词。 它意味着在每次迭代中，通过遍历对抗性示例中的每个单词，语义相似度改进只计算一次，并且替换顺序是预先确定的。然而，用原始单词进行每次替换都会改变生成的中间结果对抗性示例，从而影响原始示例和示例之间的语义相似度中间生成对抗性示例，因此只需计算语义相似度改进在每次迭代开始时确定替换顺序不准确。<br />针对上述问题，我们建议不断地替换原词，这样可以使中间生成的对抗样本与原始示例，直到违反对抗条件。具体来说，给定第t次迭代中的原始样本和对抗性例子，我们可以用以下步骤替换原始单词。<br />1.从中挑选出一个合适的替换词，公式如下：<br /><br />其中，是和之间的语义相似度，是用替换对应的单词得到的例子。是一个二值函数，定义为：<br /><br />其中，为受害者模型。如果，则等于1，否则为0。<br />2.如果，则表示可以成功攻击。我们用中对应的原始单词替换为，并重复上述步骤。<br />3.如果，则表示不能满足对抗性条件。我们终止交换过程，并返回上一步的结果。<br />经过上述过程，我们可以得到一个新的对抗样本可以保留原来的尽可能多的单词，从而提高语义相似度并降低扰动率。算法流程如附录A所示，计算复杂度和分析查询编号见附录C.1。<br />为了进一步提高生成的对抗样本的质量，我们优化了对抗样本例如，利用每个单词的同义词集。 有人可能会说我们可以直接遍历同义词集合寻找语义相似度最高的最理想同义词词并同时满足对抗条件。 然而，在大多数实际应用中场景下，查询数量通常是有限的。 为了避免遍历同义词集，我们建议通过以下两个步骤优化对抗性示例。 (1) 确定优化方案命令; (2)依次更新对抗样本。 计算复杂度分析查询编号见附录C.2。<br />在这一步中，我们的目标是确定一个合适的优化顺序。为了保证生成的对抗性实例的多样性，我们利用抽样方法来确定优化顺序。抽样方法所使用的概率分布如下。对于中的和中的，我们首先使用反拟合的词向量[29]得到它们对应的词向量，并计算它们之间的余弦距离如下：<br /><br />其中，和分别表示和的单词向量。是余弦相似度函数。然后我们用以下公式计算与在中的位置相关的概率：<br /><br />其中，是在和之间改变的单词的总数。根据概率分布，我们可以得到的优化顺序。<br />根据优化顺序，我们用同义词集按顺序更新对抗性示例。特别是，对于第t次迭代中的对抗性示例，我们用以下步骤来更新它。(1)查找过渡词；(2)估计更新方向；(3)更新对抗性示例。<br />**找到过渡词:**这一步的目的是搜索一个合理的过渡词，从而避免遍历每个更改的单词的同义词集。给定敌对的例子和当前优化单，我们随机选择同义词构造设置，使用每个元素替换和<img src="https://cdn.nlark.com/yuque/__latex/ae4caee5e098d917646e8f24d8d4d1be.svg#card=math&code=x%27_i&id=uZ0ac">，然后获得过渡词与以下公式：<br /><br />其中是用替换中相应的单词得到的例子。根据等式(7)，我们可以得到，可以使示例具有对抗性，并在一定程度上提高语义相似性，同时避免通过同义词集。此外，我们还可以在过渡词周围搜索其他可能的替换词。<br /><strong>估计更新的方向：</strong>由于过渡词来源于一个随机生成的同义词集，我们可以以一个合理的方向进一步优化它。具体来说，我们首先通过从中随机抽样个同义词来生成集合，然后获得设置，其中是用替换z中的例子。通过计算每个元素之间的语义相似度O和原始文本x，我们可以得到集，其中是x和之间的语义相似性。以类似的方式，x和旋转之间的语义相似度可以通过来计算。<br />直观地说，如果，则表示将单词向量推向会增加语义相似度，即是可以提高语义相似度的方向。如果，沿单词向量的逆方向移动单词向量可以提高语义相似性。基于上述直觉，我们通过对k个可能方向的加权平均来估计最终的更新方向u。正式地，<br /><br />其中是与方向相关的对应权重，它可以用来计算。<br /><strong>更新对抗性的示例：</strong>由于文本数据的离散性质，我们需要使用更新方向u从挑选出相应的替换词，是最大余弦相似度u和和确保满足对抗的条件。得到后，我们可以按优化顺序生成。此外，为了减少查询次数和减少扰动率，在实现程序时，我们首先使用x初始化，然后将单词逐个替换，直到满足对抗条件。<br />HQA-Attack的详细算法流程在附录B中给出。特别地，HQA-Attack首先通过随机初始化获得初始对抗样本。 然后进入主循环。 在每次迭代，HQA-Attack首先将原始单词替换回去，然后确定优化顺序，最后依次更新对抗样本。 另外，我们还提供了一些机制附录D中从决策边界角度分析HQA-Attack。<br /><strong>Experiments：</strong><br /><strong>数据集：</strong>我们在五个公共文本分类数据集先生，AG的News，Yahoo，Yelp，IMDB，以及三个自然语言推理数据集SNLI，MNLI，mMNLI上进行了实验。详细的数据集描述见附录e。我们按照之前的方法，对每个数据集取1000个测试示例进行实验。<br /><strong>基线：</strong>我们比较了三种最先进的黑盒硬标签文本对抗性攻击方法： (1) HLGA是一种利用遗传算法生成对抗性例子的硬标签对抗性攻击方法。(2)文本Hoaxer是一种硬标签对抗攻击方法，将文本数据的预算硬标签对抗攻击任务定义为连续词嵌入空间中扰动矩阵的梯度优化问题。(3)跳跃攻击是一种最近的硬标签对抗性攻击方法，它通过蒙特卡罗方法来估计梯度。<br /><strong>评估指标：</strong> 我们使用两个广泛使用的评估指标语义相似性和扰动速度。 对于语义相似度，我们利用通用序列编码器[4]来计算语义两个文本之间的相似性。 语义相似度的范围在r0,1s之间，较大者语义相似度表明攻击性能更好。 对于扰动率，我们使用对抗性示例中更改的单词数与总单词数的比率，以及扰动率越低表明结果越好。<br /><strong>受害者模型：</strong>我们采用三种广泛使用的自然语言处理模型作为受害者模型： BERT 、WordCNN和WordLSTM。所有的模型参数都取自之前的作品。我们还攻击了一些高级模型，如T5和DeBERT，结果见附录f。为了进一步验证不同算法在实际应用中的有效性，我们还尝试使用谷歌云API（<a href="https://cloud.google.com/natural-language">https://cloud.google.com/natural-language</a>）和阿里巴巴云API（<a href="https://ai.aliyun.com/nlp">https://ai.aliyun.com/nlp</a>）作为受害者模型。<br /><strong>实施细节：</strong>对于随机初始化，我们采用了与以前的方法相同的方法。在初始化之后，我们按照来执行一个预处理步骤，以删除不必要的替换词。对于超参数，我们始终为所有数据集设置r&#x3D;5和k&#x3D;5。我们的详细参数调查见附录g。此外，在优化过程中，如果我们对相同的敌对例子重新优化三次，并且没有生成新的更好的敌对例子，我们随机回到最后三到四个敌对的例子。我们不会重新优化一个敌对的例子超过两次。为了进行公平的比较，我们通过使用反拟合词向量为每个单词生成50个同义词。我们还进行了基于基于bert的同义词的实验，结果见附录H。<br />我们完全按照前面的工作，将查询预算设置为1000，即攻击者允许的查询数量为1000。由于不同的算法使用相同的随机初始化步骤来决定对抗性攻击后的预测精度，因此不同的算法具有相同的预测精度。我们的目标是生成具有较高语义相似度和较低扰动率的对抗性例子。表2和表3报告了在攻击文本分类模型时的实验结果。最好的结果用粗体突出显示。<br />如表2和表3所示，当查询限制为1000时，对于不同的数据集和任务，HQAAttack总是可以生成具有最高语义相似度和最低扰动率的对抗性示例。具体来说，对于具有短文本数据的数据集，HQA-Attack在攻击BERT、WordLSTM时，平均语义相似度分别提高了6.9%、6.5%、6.9%，平均扰动率降低了0.777%、0.832%、0.983%。对于长文本数据的数据集IMDB，HQA-Attath与攻击BERT、WordCNN和WordLSTM相比，平均语义相似度分别提高了4.5%、3.4%、2.7%，平均扰动率降低了1.426%、0.601%、0.823%。对于AG等两个以上类别的数据集，HQA-Attack在攻击BERT、WordCNN和WordLSTM时，比第二最佳方法的平均语义相似度分别提高了10.6%、8.8%、11.6%，平均扰动率降低了4.785%、3.885%、5.237%。这些结果表明，HQA-攻击可以产生高质量的对抗性攻击。<br />攻击效率是评估攻击性能的一个重要标准，因为在大多数基于DNN的NLP平台中，查询的数量是有限的。因此，我们进一步比较了所提出的HQA攻击在不同查询预算r100、300、500、700、700、1000s下的两种最新方法。如图2所示，随着查询预算的增加，所有方法的平均语义相似度都在不断增加，而所有方法的平均扰动率都在不断下降。在语义相似度和扰动率方面，HQA-Attack在所有预算中都表现得远远优于其他方法。这些结果进一步验证了我们提出的HQA-Attack能够在不同的预算限制下生成具有较高语义相似性和较低扰动率的对抗性例子。<br /><br /><br />为了进一步验证不同算法的有效性，我们尝试使用文本攻击器、跳跃攻击和HQA-攻击来攻击两个真实世界的api：谷歌云（<a href="https://cloud.google.com/naturallanguage">https://cloud.google.com/naturallanguage</a>）和阿里巴巴云（<a href="https://ai.aliyun.com/nlp">https://ai.aliyun.com/nlp</a>）。为了进一步评估生成的对抗性例子的流畅性，我们添加了困惑度（PPL）作为额外的评估度量，通过使用GPT-2 Large [31]计算。PPL越低，说明性能越好。由于谷歌和阿里巴巴只提供有限的服务预算，我们从Mr数据集中选择100个长度大于或等于20个单词的示例进行实验，并限制每种方法只能查询API 350次。表4显示了文本攻击器、跳跃攻击和HQA-攻击的结果。可以看出，与第二优结果相比，HQA-Attack分别提高了谷歌云和阿里云的语义相似度5.7%、5.1%，降低了0.062%、0.007%和PPL 9、6。我们还比较了文本攻击器、跳跃攻击和HQA-攻击在不同预算限制下的性能。结果如图3所示。我们可以得到，HQA-Attack在大多数情况下可以具有较高的语义相似度和较低的扰动率，这进一步证明了HQA-Attack优于其他基线。<br /><br /><br /><br /><br />我们利用Mr和IMDB数据集和HQA攻击数据集对Bert模型进行了人体评估实验。具体来说，对于每个数据集，我们首先随机选择50个原始样本，并使用每种对抗性攻击方法分别生成相应的50个对抗性示例。然后我们让10名志愿者注释这些样本标注类标签，并计算每种方法的平均分类精度（Acc）。直观地说，如果精度更高，这意味着生成的对抗性例子的质量更好。详细的Acc（%）结果如表5所示。结果表明，由HQA-Attack生成的对抗性实例更容易被正确分类，进一步验证了HQA-Attack在保留语义信息方面的优越性。<br />随着人工智能安全的发展，很多工作都集中在对抗性防御上攻击。 我们进一步比较了受害者模型用三种方法训练时的攻击性能有效的对抗训练策略 HotFlip 、SHIELD 和 DNE。 我们选择BERT模型作为受害者模型，将查询预算设置为1000，然后在AG 数据集。 我们还使用困惑度（PPL)作为附加评估指标来判断生成的对抗性示例的流畅性。 表6显示了攻击性能。 我们可以观察到与其他强基线相比，HQA-Attack 也可以获得最好的结果。&lt;br &#x2F;<br /><br />为了研究不同成分的有效性，我们对五个文本进行了消融研究攻击WordCNN 时的分类数据集。 结果如表7所示。随机初始化是指仅通过随机初始化步骤生成的对抗样本。 不带&#x2F;不带替换是指HQA-Attack模型无需替换原词后退一步。w&#x2F;o Optimizing 意味着 HQA-Attack 模型随机选择一个可以保持示例对抗性作为替换，在不优化的情况下将原始单词替换回去对抗性的例子。 很容易发现所有模块都对模型有贡献，这验证了替换原始单词的后退步骤很有用，优化对抗性示例步骤是也是不可或缺的。 为了进一步证明我们提出的单词回替换的有效性我们在附录 I 中添加了一些额外的实验。我们还列出了 HQA-Attack 生成的一些具体对抗示例，如附录 J 所示。这些示例进一步证明了我们提出的 HQA-Attack 模型可以生成高质量的黑盒硬标签对抗模型仅具有小扰动的示例。</p><p><strong>Conclusion：</strong><br />在本文中，我们提出了一种名为HQA-Attack的新方法，以在黑盒硬标签设置中制作高质量的文本对抗性示例。HQAAttack通过替换原词，可以大大降低扰动率。HQA-Attack利用剩余更改词的同义词集对对抗性示例进行优化，可以提高语义相似度，减少查询预算。大量的实验结果表明，所提出的HQA-Attack方法能够生成具有高语义相似度、低扰动率、较少查询数的高质量对抗性实例。在未来的工作中，我们计划尝试更多的优化策略来完善模型，从而进一步提高文本对抗性攻击的性能。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Text-attack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LimeAttack_ Local Explainable Method for Textual Hard-Label Adversarial Attack</title>
      <link href="/2024/04/21/limeattack-local-explainable-method-for-textual-hard-label-adversarial-attack/"/>
      <url>/2024/04/21/limeattack-local-explainable-method-for-textual-hard-label-adversarial-attack/</url>
      
        <content type="html"><![CDATA[<span id="more"></span><p><strong>Summary：</strong><br />Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt model internal information (gradients or confidence scores) to generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experiments show that LimeAttack achieves the better attacking performance compared with existing hard-label attack under the same query budget. In addition, we evaluate the effectiveness of LimeAttack on large language models and some defense methods, and results indicate that adversarial examples remain a significant threat to large language models. The adversarial examples crafted by LimeAttack are highly transferable and effectively improve model robustness in adversarial training.</p><p><strong>Background：</strong><br />深度神经网络（DNN）广泛应用于自然语言处理领域并取得了巨大成就成功（Kim 2014；Devlin 等人 2019；Minaee 等人 2021；Hochreiter 和Schmidhuber 1997）。 然而，DNN 很容易受到对抗性示例的影响，而这些示例被正确分类样本因一些轻微的扰动而改变（Jin et al. 2020；帕佩诺特等人。 2017年； Kurakin、Goodfellow 和 Bengio 2016）。这些对抗性扰动是人类无法察觉的但可能会误导模型。 严重的对抗性例子威胁 DNN 的鲁棒性和可靠性，尤其是在一些安全关键型应用（例如自动驾驶和有毒文本检测（Yang et al. 2021；Kurakin, Good Fellow 和 Bengio 2018））。 因此，对抗性例子计算机视觉、自然语言处理等领域的对抗性攻击和防御引起了极大的关注演讲（Szegedy 等人，2013 年；Carlini 和 Wagner，2018 年；Yu等人。 2022）。 制作文本对抗性更具挑战性由于语言的离散性质以及词汇、语义和流畅性限制的存在。<br />根据不同场景，进行文本对抗攻击可以简单分为白盒攻击、基于分数的攻击和硬标签攻击。 在白盒设置中，攻击者利用模型的参数和梯度来生成对抗性示例（Goodman、Zhonghou et al. 2020；江等人。 2020）。 基于分数的攻击仅采用类别概率或置信度分数来制作对抗性示例（Jin等人。 2020； 李等人。 2020； 马、史、管2020； 朱，赵和吴 2023）。 然而，由于 DNN 是通过以下方式部署的，这些攻击方法在现实中表现不佳应用程序编程接口（API）和攻击者无法访问模型的参数、所有标签的梯度或概率分布（Ye 等人，2022b）。 相比之下，在硬标签场景下，模型的内部结构，梯度、训练数据甚至置信度分数都是不可用的。 攻击者只能查询黑盒受害者模型并得到离散的预测标签，这更具有挑战性和现实性。 此外，最真实的模型（例如HuggingFace API、OpenAI API）通常有限制关于通话次数。 事实上，对抗性例子攻击设置是带有微小模型查询的硬标签。<br />一些硬标签攻击算法已经被提出（Yu等人。 2022 年； 叶等人。2022b； 马赫什瓦里、马赫什瓦里和普地2021； 叶等人。 2022a）。 他们遵循两阶段策略：i）通过随意用同义词替换几个原始单词来生成低质量的对抗性示例，以及然后 ii) 采用复杂的启发式算法（例如遗传算法）来优化对手的扰动。 所以，这些攻击方法通常需要大量查询，并且攻击成功率和对抗样本的质量是受到对手初始化的限制。 相反，基于分数的攻击根据删除一个单词后置信度分数发生变化。 单词重要性排序通过优先提高攻击效率攻击对模型有重大影响的词预测（Jin 等人，2020）。 然而，基于分数的攻击无法计算硬标签设置中的单词重要性因为删除一个标记几乎不会改变离散预测标签。 因此，我们要研究这样一个问题：如何计算硬标签中的单词重要性排名设置以提高攻击效率？<br />实际上，单词重要性排名可以揭示决策边界以确定更好的攻击路径，但现有的硬标签算法忽略了这些有用的信息因为它很难获得。 受到当地可解释的启发方法（Ribeiro、Singh 和 Guestrin 2016；Lundberg 和李2017; 什里库马尔等人。 2016）对于 DNN，有 10 个用于解释黑盒模型的输出，目标估计良性样本的标记敏感性。 之前的研究（Chai et al. 2023）试图简单地替换基于删除的方法和局部可解释的方法来计算基于分数的攻击中的单词重要性。 然而，在附录B，我们通过实验验证了本地可解释的方法没有显着优势在基于分数的场景中优于基于删除的方法。 因为模型输出的概率分布是可用，每个词对输出的影响可以是基于删除的方法很好地反映了这一点。 因此，与基于分数的攻击相比，我们认为局部可解释方法在硬标签攻击中可以发挥更大的优势基于删除的方法是无用的。 我们采用最多基本且直接的本地可解释方法，即石灰。 LIME 简单易懂，更符合使用基于分数的攻击中提出的基于删除的方法，因为我们的目标是弥合基于分数的攻击之间的差距通过引入可解释性方法来攻击和硬标签攻击。 事实上，局部可解释方法与模型无关，适合对硬标签攻击进行单词重要性估计。 然而，有以下几种情况将 LIME 应用于硬标签攻击的困难：1）如何在极小的查询预算下分配 LIME 和搜索查询以达到最佳效果。 2）如何建立映射在没有模型 Logits 输出的情况下，LIME 与对抗性样本中单词重要性之间的关系。 3) 如何取样在扰动执行期间合理地实现最优结果。 在随后的课程中我们将详细解释如何来解决这些困难。<br /><strong>Contributions：</strong><br />在这项工作中，我们提出了一种新的硬标签攻击算法，称为极限攻击。LIME在硬标签攻击中的应用是受到基于分数的攻击删除方法的启发。我们验证了内外攻击路径在硬标签攻击中的有效性，然后许多优秀的基于分数的攻击可以为硬标签攻击提供更多的见解。为了评估攻击的性能和效率，我们将Lime攻击与其他硬标签攻击进行了比较，并将一些基于分数的攻击作为7个常见数据集上的两个NLP任务的参考。我们还评估了目前最先进的大型语言模型（例如，ChatGPT）的限制eattack。实验表明，在微小的查询预算下，极限攻击比其他基线获得了最高的攻击成功率。我们的贡献总结如下：</p><ul><li>总结了现有硬标签攻击的不足，并应用LIME将分数基攻击与硬标签攻击连接起来，验证了内外攻击路径在硬标签攻击中的有效性。</li><li>大量实验表明，在小预算的查询情况下，有限攻击比现有的硬标签攻击算法具有更高的攻击成功率。与此同时，由极限攻击制作的对抗性例子是高质量的，人类很难区分。</li><li>此外，我们还对当前最先进的大型语言模型进行攻击和评估。结果表明，敌对的例子仍然是对大型语言模型的一个重大威胁。我们还增加了防御方法的攻击性能，以及攻击成功率和扰动率的收敛结果。</li></ul><p><strong>Method：</strong><br /><strong>硬标签对抗性攻击</strong><br />在硬标签设置中，攻击者只能查询受害者模型并获得离散预测标签。 因此，硬标签设置更具实用性和挑战性。 现有的硬标签攻击包含两阶段策略，即对手初始化和扰动优化。 HLBB（Mahesh wary、Maheshwary 和 Pudi 2021）初始化对抗性示例并采用遗传算法来优化扰动。 TextHoaxer（Ye 等人，2022b）和 LeapAttack（Ye 等人，2022b）2022a) 利用语义相似性和扰动率作为优化目标是在连续词嵌入空间中搜索更好的扰动矩阵。 文本黑客（于等人。 2022）采用混合本地搜索算法和单词从攻击历史中学习的重要性表来指导本地搜索。 这些攻击方法往往需要大量的查询降低扰动率，降低攻击成功率对手的质量受到初始化的限制。 因此，在这项工作中，我们尝试制作一个对抗性示例直接来自良性样本。 这种方法可以生成具有更少查询的高质量对抗性示例。<br /><strong>局部可解释方法</strong><br />为了提高 DNN 可解释性并辅助决策，已经提出了各种解释 DNN 的方法并大致分为全局或局部可解释方法。 全局可解释方法侧重于模型本身使用有关模型架构的整体知识和参数。 相反，本地方法适合简单的和可解释模型（例如决策树）到单个输入衡量每个代币的贡献。 详细点，本地可解释的方法（Lundberg 和 Lee 2017；Shrikumar等人。 2016年； ˇStrumbelj 和 Kononenko 2014) 将所有通过定义线性可解释性模型来输入标记假设输入中每个令牌的贡献是添加剂。 这也称为附加特征归因方法。 在本文中，局部可解释模型不可知解释（LIME）（Ribeiro、Singh 和 Guestrin 2016）是用于计算单词重要性，这是一个基本的和代表性的局部可解释方法。 直觉LIME是通过删除来生成很多邻域样本良性示例中的一些原始单词。 这些样品然后用于训练线性模型，其中特征等于良性样本中的单词数。该线性模型的参数近似于每个单词的重要性。 由于 LIME 与模型无关，因此适合硬标签攻击。<br /><strong>现有硬标签攻击的局限性</strong><br />为了直观地比较两者的区别LimeAttack 和现有的硬标签攻击算法，我们创建图 3 中的攻击搜索路径可视化。LimeAt Tack 的搜索路径由绿线表示，它们从内部移动到外部。 LimeAttack 利用本地可解释的方法来学习单词重要性排名，并从良性样本中迭代生成对抗性示例。这有助于 LimeAttack 找到最近的决策边界方向，并且攻击关键字的模型查询成本更少优先。 相比之下，以前的硬标签攻击算法的搜索路径是用蓝线表示的，它们从外向内移动。 这些算法通常从随机初始化的对抗性示例开始，通过最大化初始化示例和良性样本之间的语义相似性来优化扰动，这需要大量的模型查询来实现低扰动速度。 此外，他们的攻击成功率和对手质量也受到对手初始化的限制。<br /><br /><strong>问题公式</strong><br />给定一个n个单词及其地面真相标签Y，一个对立的例子是通过用同义词替换一个或多个原始词来误导受害者模型F而制作的。i.e.,<br /><br />D（·，·）是一个编辑距离，测量良性样本和对抗性例子之间的修改：<br /><br />E（·，·）是一个二进制变量，如果，则等于0，否则等于1。一个高质量的对抗性例子应该与良性样本相似，人类读者应该很难区分出其区别。极限攻击属于硬标签攻击，它与模型的参数、梯度或置信度分数无关。攻击者只能查询受害者模型，获得预测标签。<br /><strong>所提出的极限攻击算法</strong><br />总体流程图如图2所示。限制攻击遵循两个步骤，即单词的重要性排序和扰动执行。<br /><strong>Word重要性排名</strong><br />给定一个有n个单词X的句子，我们假设所有单词的贡献都是可加性的，并且它们的和与模型的预测呈正相关。如图2所示，我们从一个良性的例子X中随机替换一些单词为‘[MASK]’，生成一些邻域样本。通常，有更多单词的句子通常需要更多的邻居样本来近似单词的重要性。因此，我们保持邻域样本的数量与令牌的数量保持一致。然后，我们将X提供给受害者模型F，以获得离散的预测标签。随后，我们将拟合一个线性可解释性模型来对这些邻域样本进行分类：<br /><br />其中θ是线性模型的参数，I（·，·）是一个二进制变量，如果X中的单词xi，则等于1，否则为0。因此，参数反映了没有单词xi的变化，并近似于单词的重要性。在附录O中，我们通过实验验证了在微小的查询预算下，线性模型（如LIME）与一些先进的解释方法（如SHAP）或非线性模型（如决策树）具有相同的效果。形状模型或非线性模型也具有较高的计算复杂度。一些先进的解释方法或非线性模型的优势只有在有大量的邻域样本和查询时才会反映出来。<br />详细地说，我们将每个邻域样本Xi‘转换为二进制向量Vi’。如果在Xi‘中去掉了原词，则其在Vi’中对应的向量维数为0，否则为1。因此，Vi‘与Xi’的长度相同，这是良性例子的长度。一个良性的例子X也被转换为V。有时邻域样本不一定是线性可分的，LIME采用高斯核对每个样本的损失进行加权，以收集最接近原始样本的点，这有助于线性拟合。我们根据每个邻域样本与良性样本的距离（Ribeiro，Singh，和Guestrin 2016）给予其权重。<br /><br />其中，d（.，.）是一个距离函数。我们采用余弦相似度作为距离度量。<br /><br /><br />最后，我们计算了最优参数θ∗：<br /><br />其中Ω(θ)是参数的非零，这是对线性模型复杂性的度量。优化θ后，每个单词xi的重要性等于θi。LIME可以看作是原始样本中模型决策边界的近似值。这些参数可以解释为边际，边际越大，这个词在近似决策边界时的重要性就越大。我们将首先使用NLTK2过滤掉停止词，并计算每个单词的重要性。确保极限攻击产生了高质量的对抗性例子，而不仅仅是消极的例子。我们只采用同义词替换策略，通过在反拟合嵌入空间中选择最前k个最近的同义词，为每个单词xi构造同义词候选集C（xi）（Mrkˇsi‘cetal.2016）。此外，我们在附录一中展示了人类评估的结果和更多定性的对抗性例子。<br /><strong>扰动执行</strong> <br />对抗性例子生成是一个组合优化问题。 基于分数的攻击通过选择引起最大变化的标记进行迭代每次都在模型的 logits 中。 但没有这样的信息在硬标签攻击中。因此，我们只能依赖对抗样本与原始样本之间的相似性用于迭代的样本。 问题是相似度和攻击成功率并不完全线性相关。 作为如表7所示，每次贪婪地选择相似度最低的对抗样本并不能保证最终的攻击成功率是最优的。 我们希望每个采样均匀分布以平衡攻击成功率和语义相似度。对于每个起源词xi，我们将其替换为c∈C（xi）来生成一个对抗示例，然后我们计算良性样本X之间的语义相似性通过通用句子编码器（USE）3。我们首先根据相似性对候选对象进行排序，并每次抽样b个对抗性的例子，以进入下一次迭代。详细地说，我们制定了以下采样规则： (1)采样b&#x2F;3个语义相似度最高的对抗性样本。(2)抽样具有语义相似度最低的b&#x2F;3个对抗性样本。(3)随机抽取剩余的敌对性样本中的b&#x2F;3个。附录C和H总结了超参数b的分析和极限攻击算法的分析。</p><p><strong>Experiments：</strong><br />附录D和附件E中列出了对极限攻击的可转移性和对抗性训练的分析。<br /><strong>任务、数据集和模型</strong><br />我们采用了7个常见的数据集，如先生（庞和李2005）、SST-2（Sochere人2013）、AG（张、赵和乐村2015）和雅虎（Yoo等人2020）进行文本分类。SNLI（Bowman等人，2015年）和MNLI（威廉姆斯、Nangia和鲍曼，2018年）的文本含义，其中MNLI包括一个匹配版本（MNLIm）和一个不匹配版本、（MNLImm）。此外，我们还训练了三种神经网络作为受害者模型，包括CNN（Kim，2014年）、LSTM（霍克雷特和施米德胡伯，1997年）和BERT（Devlin等人，2019年）。模型的参数和数据集的详细信息列于附录A中。<br /><strong>基线</strong><br />我们选择以下现有的硬标签攻击算法作为我们的基线：HLBB（Maheshwary、Mahesh wary 和 Pudi 2021）、TextHoaxer（Ye 等人，2022b）、Leap Attack（Ye 等人，2022a）和 TextHacker（Yu 等人，2022）。 此外，我们还收录了一些经典的基于分数的攻击算法，例如TextFooler (TF) (Jin等人。 2020)、PWWS (Ma, Shi, andguan 2020) 和 Bert Attack (Li et al. 2020) 供考，其中获得了额外的攻击的置信度分数并在TextAttack 框架（Morris 等人，2020）。<br /><strong>自动评估指标</strong><br />我们使用四个指标来评估攻击性能：攻击成功率（ASR）、扰动率（Pert）、语义相似度（Sim）和查询数（查询）。具体来说，给定一个数据集由N个样本和相应的标签，攻击成功率对抗攻击方法，生成对抗的例子(X)给定输入X攻击受害者模型F，定义为（王et al. 2021）：<br /><br />扰动率是替换数与原始token数的比例，这已在Eq 2中定义。语义相似度是通过通用句子编码器（USE）来度量的。大多数论文（马赫什瓦里、马赫什瓦里和普迪，2021年；Ye等人，2022年a）已经采用了USE。为了保持一致性和促进可比性，我们也使用了USE。查询号是在攻击期间的模型查询数。模型的鲁棒性与攻击成功率成反比，而扰动率和语义相似性共同揭示了对抗性例子的质量。查询号显示了攻击的效率。<br /><strong>实施细节</strong><br />我们设置核宽度 σ &#x3D; 25，邻域样本的数量等于良性样本的数量令牌，光束大小 b &#x3D; 10。为了公平比较，所有基线遵循相同的设置：选择同义词来自反向安装的嵌入空间和每个嵌入空间的数量候选集 k &#x3D; 50，同样采样 1000 个文本攻击的基线。 结果是五次运行的平均值不同的种子（1234、2234、3234、4234和5234）来消除随机性。 为了提高对抗质量例如，如果每个的扰动率都相同，则攻击成功对抗性例子不到10%。 我们设置一个小查询硬标签攻击的预算为 100，对应于现实世界的设置。 （例如，HuggingFace 免费推理API 通常将调用次数限制为每分钟 200 次。）<br /><strong>实验结果</strong><br /><strong>攻击性能：</strong>表1和表2显示，Lime攻击在文本分类和文本隐含任务上优于现有的硬标签攻击，在SST-2、AG和MNLI等数据集上获得了更高的攻击成功率和更低的扰动率。与现有的硬标签攻击需要许多查询来优化扰动不同，Lime攻击采用了一种本地可解释的方法来计算单词的重要性排序，并首先攻击关键词。这种方法可以生成具有高攻击成功率的对抗性示例，即使是在很小的查询预算下。附录G包括一个t检验和与其他方法相比的限制攻击的成功率的平均值和方差。在附录K和L中，我们列出了Lime攻击和几种基于分数的攻击之间的语义相似性和比较结果。<br /><strong>查询预算：</strong>如图3所示，在不同的查询预算下，LimeAttatack仍然保持着稳定的攻击成功率和更平滑的攻击曲线，这意味着无论查询预算是高或低，Lime攻击通常都具有稳定和优秀的攻击性能。扰动率的变化趋势见附录n，通过比较低查询和高查询预算下的攻击性能，可以提供更全面的评价。然而，不考虑查询预算的攻击是一种比较理想的情况，它显示了攻击算法的上限。大量的查询是昂贵的，我们认为在低查询预算下的攻击性能更实用。我们还在附录N中列出了在查询预算为2000下的不同攻击的一些攻击成功率和扰动率。<br /><br /><strong>对手的品质</strong>：高质量的对抗样本应该既流畅又具有上下文感知能力，同时也与良性样本相似以逃避人类检测。 我们利用语言工具4并用于检测语法错误并测量语义相似性。 如表3所示，LimeAttack 的扰动率和语法性最低错误，虽然其语义相似度低于HLBB，TextHoaxer 和 LeapAttack。 因为这些方法需要在攻击过程中考虑相似性，因此 LimeAttack与其他方法相比，相似度较低。 考虑到所有指标中，LimeAttack 仍然占据主导地位。 为了直观地比较对抗性例子的质量，一些定性的附录一提供了示例。<br /><br /><br /><strong>大型语言模型的评估</strong>：大语言模型（LLM），也称为基础模型（Bom masani et al. 2021），在以下方面取得了令人印象深刻的表现各种自然语言处理任务。 然而，它们对对抗性例子的鲁棒性仍不清楚（Wang 等人，2023）。 为了评估 LimeAttack 对法学硕士的有效性，我们选择了一些流行的型号，例如DeBERTa-L（Kojima等人。 2022）、BART-L（Lewis 等人，2019）、Flan-T5（Raffel<br />等人。 2020)、GPT-3 (text-davinci-003) 和 ChatGPT (gpt-3.5-turbo）（Brown 等人，2020）。 由于API调用有限，我们从 MR 数据集中抽取 100 个文本，并攻击这些模型的零样本分类任务。 如表4所示，LimeAttack 成功攻击了大多数 LLM查询预算。 尽管这些模型具有很高的准确度零样本任务，它们对对抗性例子的鲁棒性仍然需要改进。 ChatGPT 和 T5-L 更稳健到对抗性的例子。 受害者模型的稳健性与原点精度有关。 原点精度越高，受害者模型防御对抗性的能力越强例子。 其他硬标签攻击的进一步分析和实验细节在附录 F 中讨论。<br /><br /><br /><strong>对防御方法的攻击性能</strong> <strong>：</strong>评估LimeAttack 对防御方法的有效性，我们使用 A2T（Yoo 和 Qi 2021）和 ASCC（Dong 等人，2021）增强BERT对SNLI的防御能力，并在此基础上进行了攻击实验。 如表5所示，LimeAttack仍然有一定的攻击效果和结果这些防御方法的其他基线。 更多攻击性能防御方法列于附录M。<br /><strong>消融研究</strong><br /><strong>单词重要性排名的影响</strong>： 为了验证单词重要性排序的有效性，我们删除了单词重要性排名策略，而是随机选择单词进行扰动以评估其有效性。 表6表明在没有单词重要性排名的情况下，攻击MR 和 SST 的成功率分别下降 9% 和 6%分别是2个数据集。 此外，对抗性例子随机选择产生的扰动率更高并需要更多查询。 这表明了重要性引导 LimeAttack 聚焦的单词重要性排名关键词，以更低的成本实现更有效的攻击扰动率。<br /><strong>抽样规则的效果：</strong>为了验证LimeAttack抽样规则的有效性，我们将用三种常见的抽样规则之一替换该策略： (1)选择b个语义相似度最高的对抗性例子，(2)选择b个语义相似度最低的对抗性例子，或(3)随机选择b个对抗性例子。表7的结果表明，Lime攻击具有较高的攻击成功率和较低的扰动率，优于其他采样规则。此外，它还具有类似的（第二高的）语义相似度和查询的数量。<br /><br /><br /><strong>人工评价</strong><br />我们选择了 200 个 BERT-MR 对抗样本。 每个对抗性例子都由两名人类评委进行评估语义相似度、流畅度和预测准确性。 这整个人类评价与TextFooler一致（Jin等人。 2020）。 具体来说，我们要求人类评委打出 5 分Likart 量表（1-5 对应于非常不流利&#x2F;相似，不流畅&#x2F;相似、不确定、流畅&#x2F;相似、非常流畅&#x2F;相似分别）评价相似度和流畅度对抗性样本和良性样本。 结果是如表8所示，语义相似度为4.5，即对抗样本与原始样本相似。 这里的预测准确度是让人类能够预测出什么这句话的标签是（例如它是肯定的还是否定的情绪分析）。 76.7% 表示大多数是敌对的示例与原始样本具有相同的属性人类的视角却错误的受害者模型。<br /></p><p><strong>Conclusion：</strong><br />在本工作中，我们总结了以往基于分数的攻击和硬标签攻击，并提出了一种新的硬标签攻击算法，称为极限攻击。极限攻击采用一种局部可解释的方法来近似计算单词的重要性排序，然后利用波束搜索，以微小的查询预算生成高质量的对抗性示例。实验表明，极限攻击的攻击成功率高于其他硬标签攻击的攻击成功率。此外，我们还评估了极限攻击在大型语言模型和一些防御方法上的攻击性能。极限攻击所制作的对抗性实例具有高质量、高可转移性，提高了受害者模型在对抗性训练中的鲁棒性。极限攻击已经验证了在硬标签中的内外攻击路径的有效性。然后，许多优秀的基于分数的攻击可能会提供更深入的硬标签攻击。<br /></p>]]></content>
      
      
      
        <tags>
            
            <tag> Text-attack </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
